// apps/customer-backend/src/modules/chat/chat.ai.service.js
// ‚úÖ AI Service: Wrapper cho OpenAI API (Chat Completion & Vision)

import OpenAI from "openai";
import { config } from "../../config/env.config.js";
import { Logger } from "../../shared/utils/index.js";

export class ChatAiService {
  constructor() {
    if (!config.apiKeys?.openai) {
      Logger.warn("[ChatAiService] OPENAI_API_KEY is not configured.");
      this.openai = null;
    } else {
      this.openai = new OpenAI({
        apiKey: config.apiKeys.openai,
      });
    }

    this.model = "gpt-4o-mini"; // Model m·∫∑c ƒë·ªãnh
    this.visionModel = "gpt-4o"; // Model cho Vision
  }

  /**
   * Get Chat Completion v·ªõi tools support
   * @param {Array} messages - Array of message objects
   * @param {Array} tools - Tool definitions (optional)
   * @param {Object} context - Context object (optional)
   * @param {Function} onToken - Callback for streaming tokens (optional)
   * @returns {Promise<Object>} OpenAI response
   */
  async getCompletion(messages, tools = [], context = {}, onToken = null) {
    if (!this.openai) {
      throw new Error("OpenAI API key is not configured");
    }

    try {
      const requestOptions = {
        model: this.model,
        messages: messages,
        temperature: 0.7,
      };

      // Th√™m tools n·∫øu c√≥
      if (tools && tools.length > 0) {
        requestOptions.tools = tools;
        requestOptions.tool_choice = "auto";
      }

      // Streaming n·∫øu c√≥ callback
      if (onToken) {
        requestOptions.stream = true;

        const stream = await this.openai.chat.completions.create(
          requestOptions
        );
        let fullContent = "";

        let buffer = "";
        const BATCH_SIZE = 10; // TƒÉng l√™n 10 chars ƒë·ªÉ gi·∫£m s·ªë l·∫ßn emit

        for await (const chunk of stream) {
          const delta = chunk.choices[0]?.delta;
          if (delta?.content) {
            fullContent += delta.content;
            buffer += delta.content;

            // üéØ SIMPLE CHUNKING: Emit m·ªói 10 chars ho·∫∑c khi g·∫∑p newline
            // Frontend s·∫Ω x·ª≠ l√Ω vi·ªác t√¨m safe breakpoint
            if (buffer.length >= BATCH_SIZE || delta.content.includes("\n")) {
              Logger.info(
                `[AI] Sending chunk: "${buffer.substring(0, 20)}..." (${
                  buffer.length
                } chars)`
              );
              onToken(buffer);
              buffer = "";
            }
          }
        }

        // Emit ph·∫ßn c√≤n l·∫°i
        if (buffer) {
          onToken(buffer);
        }

        // Tr·∫£ v·ªÅ format gi·ªëng non-streaming response
        return {
          choices: [
            {
              message: {
                role: "assistant",
                content: fullContent,
              },
            },
          ],
        };
      }

      // Non-streaming
      const response = await this.openai.chat.completions.create(
        requestOptions
      );
      return response;
    } catch (error) {
      Logger.error("[ChatAiService] getCompletion error:", error);
      throw error;
    }
  }

  /**
   * Get Completion v·ªõi custom prompt (kh√¥ng d√πng tools)
   * @param {Array} messages - Array of message objects
   * @param {String} customPrompt - Custom system prompt
   * @returns {Promise<Object>} OpenAI response
   */
  async getCompletionWithCustomPrompt(messages, customPrompt) {
    if (!this.openai) {
      throw new Error("OpenAI API key is not configured");
    }

    try {
      // Th√™m custom prompt v√†o messages
      const messagesWithPrompt = [
        ...messages,
        { role: "system", content: customPrompt },
      ];

      const response = await this.openai.chat.completions.create({
        model: this.model,
        messages: messagesWithPrompt,
        temperature: 0.7,
      });

      return response;
    } catch (error) {
      Logger.error(
        "[ChatAiService] getCompletionWithCustomPrompt error:",
        error
      );
      throw error;
    }
  }

  /**
   * Get Vision Completion (Ph√¢n t√≠ch ·∫£nh)
   * @param {String} imageUrl - URL c·ªßa ·∫£nh c·∫ßn ph√¢n t√≠ch
   * @param {String} prompt - Prompt cho AI
   * @param {Object} context - Context object (optional)
   * @returns {Promise<String>} Analysis text
   */
  async getVisionCompletion(imageUrl, prompt, context = {}) {
    if (!this.openai) {
      throw new Error("OpenAI API key is not configured");
    }

    try {
      const response = await this.openai.chat.completions.create({
        model: this.visionModel,
        messages: [
          {
            role: "user",
            content: [
              {
                type: "text",
                text: prompt,
              },
              {
                type: "image_url",
                image_url: {
                  url: imageUrl,
                },
              },
            ],
          },
        ],
        max_tokens: 500,
        temperature: 0.7,
      });

      const analysis =
        response.choices[0]?.message?.content || "Kh√¥ng th·ªÉ ph√¢n t√≠ch ·∫£nh n√†y.";
      return analysis;
    } catch (error) {
      Logger.error("[ChatAiService] getVisionCompletion error:", error);
      throw error;
    }
  }
}
